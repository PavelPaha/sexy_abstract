\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\graphicspath{ {./images/} }


\title{Predict future sale}


\author{
 Ziyue Qi \\
  School of Coumputing and Information\\
  University of Pittsburgh\\
  Pittsburgh, PA 15213 \\
  \texttt{ziq2@pitt.edu} \\
  %% examples of more authors
   \And
 Zixuan Lu \\
  School of Coumputing and Information\\
  University of Pittsburgh\\
  Pittsburgh, PA 15213 \\
  \texttt{ZIL50@pitt.edu} \\
  \And
 Yuchen Lu \\
  School of Coumputing and Information\\
  University of Pittsburgh\\
  Pittsburgh, PA 15213 \\
  \texttt{yul217@pitt.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
\maketitle
\begin{abstract}
ABoba
\end{abstract}


% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}

% SEXY_ABSTRACT_BEGIN
\section{Introduction}

We hypothesize that the reason Policy Distillation does not improve through trial an error is that it
trains on data that does not show learning progress. Current methods either learn policies from data
that contains no learning (e.g. by distilling fixed expert policies) or data with learning (e.g. the replay
buffer of an RL agent) but with a context size that is too small to capture policy improvement.
Our key observation is that the sequential nature of learning within RL algorithm training could,
in principle, make it possible to model the process of reinforcement learning itself as a causal
sequence prediction problem. Specifically, if a transformerâ€™s context is long enough to include policy
improvement due to learning updates it should be able to represent not only a fixed policy but a policy
improvement operator by attending to states, actions and rewards from previous episodes. This opens
the possibility that any RL algorithm can be distilled into a sufficiently powerful sequence model
such as a transformer via imitation learning, converting it into an in-context RL algorithm.
We present Algorithm Distillation (AD), a method that learns an in-context policy improvement
operator by optimizing a causal sequence prediction loss on the learning histories of an RL algorithm.
AD has two components. First, a large multi-task dataset is generated by saving the training histories
of an RL algorithm on many individual tasks. Next, a transformer models actions causally using the
preceding learning history as its context. Since the policy improves throughout the course of training
of the source RL algorithm, AD is forced to learn the improvement operator in order to accurately
model the actions at any given point in the training history. Crucially, the transformer context size
must be sufficiently large (i.e. across-episodic) to capture improvement in the training data. The full
method is shown in Fig. 1.

% SEXY_ABSTRACT_END

\bibliographystyle{unsrt}
%\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.


% SEXY_REFERENCES
\begin{thebibliography}{1}
% SEXY_REFERENCES_BEGIN


% SEXY_REFERENCES_END
\end{thebibliography}


\end{document}
